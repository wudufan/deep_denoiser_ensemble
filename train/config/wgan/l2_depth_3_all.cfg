[IO]
datadir=/home/dwu/trainData/deep_denoiser_ensemble/data/mayo_2d_3_layer_mean/
manifest=/home/dwu/trainData/deep_denoiser_ensemble/data/mayo_2d_3_layer_mean/manifest.csv
outdir=/home/dwu/trainData/deep_denoiser_ensemble/train/mayo_2d_3_layer_mean/
train=
valid=L291,L143,L067
target=dose_rate_1
#source=dose_rate_2,dose_rate_3,dose_rate_4,dose_rate_5,dose_rate_6,dose_rate_7,dose_rate_8,dose_rate_9,dose_rate_10,dose_rate_11,dose_rate_12,dose_rate_13,dose_rate_14,dose_rate_15,dose_rate_16
source=dose_rate_2,dose_rate_4,dose_rate_8,dose_rate_12,dose_rate_16
# the final output directory will be os.path.join(outdir, tag)
tag=l2_depth_3_wgan/all
checkpoint=
# whether to remove all the previous log files for a new run
relog=1

[Training]
device=0
epoch=5
start_epoch=0
# patch shape in nx, ny, nz
imgshape=64,64,1
batchsize=100
lr=0.0001
lr_discriminator=0.0001
# The learning rate will be reduced linearly to lr_reduction_end * lr every lr_reduction_interval iterations
lr_reduction_end=0.1
lr_reduction_interval=1000
save_model_interval=1
output_interval = 10

[Testing]
# patch shape in nx, ny, nz
imgshape=640,640,1
stepsize=448,448,1

[Network]
down_features=64,64,64
bottleneck_features=64
# use blank to keep symmetry with the downsample path
up_features=
strides=1,1,1
use_adding=1
lrelu=0.2

[Discriminator]
features=64,128,256
fc_features=1024
strides=2,2,2
lrelu=0.2
dropouts=0.5
layer_norm=0

[WGAN]
l2_weight=50
gp_weight=10
discriminator_steps=1

[Augmentation]
flipx=1
flipy=1

[Window]
vmin=
vmax=